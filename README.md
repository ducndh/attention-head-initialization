# attention-head-initialization
This repo containing code trying to understand the dynamic of attention weight with different initialization
